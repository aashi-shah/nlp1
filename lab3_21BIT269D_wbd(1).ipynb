{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 1: Tokenization with Basic Split</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using split(): ['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language,', 'library,', 'and', 'purpose', 'of', 'modeling.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization with basic split by whitespace\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. \n",
    "We can choose any method based on language, library, and purpose of modeling.\"\"\"\n",
    "tokens = text.split()\n",
    "print(\"Tokens using split():\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split() method is basic and splits tokens by whitespace but is limited since it doesn't handle punctuation properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 2: Tokenization using Regular Expressions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens using regex: ['Characters', 'like', 'periods', 'exclamation', 'points', 'and', 'newline', 'characters', 'are', 'used', 'to', 'separate', 'sentences', 'But', 'one', 'drawback', 'with', 'split', 'is', 'that', 'we', 'can', 'only', 'use', 'one', 'separator', 'at', 'a', 'time', 'So', 'sentence', 'tokenization', 'won', 't', 'be', 'foolproof', 'with', 'split']\n",
      "Sentence tokens: ['Characters like periods, exclamation points, and newline characters are used to separate sentences', '\\nBut one drawback with split() is that we can only use one separator at a time', \"So sentence tokenization won't be foolproof with split().\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"Characters like periods, exclamation points, and newline characters are used to separate sentences. \n",
    "But one drawback with split() is that we can only use one separator at a time! So sentence tokenization won't be foolproof with split().\"\"\"\n",
    "\n",
    "# Tokenizing words, ignoring punctuation\n",
    "tokens = re.findall(r\"\\w+\", text)\n",
    "print(\"Tokens using regex:\", tokens)\n",
    "\n",
    "# Tokenizing sentences using regex\n",
    "sentences = re.compile(r'[.!?]\\s').split(text)\n",
    "print(\"Sentence tokens:\", sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using re.findall to handle tokenization of words, which ignores punctuation. For sentence tokenization, a regular expression is used to split by periods, exclamation marks, and question marks followed by a space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 3: Tokenization using NLTK</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens with NLTK: ['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language', ',', 'library', ',', 'and', 'purpose', 'of', 'modeling', '.']\n",
      "Sentence tokens with NLTK: ['There are multiple ways we can perform tokenization on given text data.', 'We can choose any method based on language, library, and purpose of modeling.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aashi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Word tokenization using NLTK\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. \n",
    "We can choose any method based on language, library, and purpose of modeling.\"\"\"\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Word tokens with NLTK:\", tokens)\n",
    "\n",
    "# Sentence tokenization using NLTK\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence tokens with NLTK:\", sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK word_tokenize function handles punctuation, making it more effective for NLP tasks. sent_tokenize is also used for sentence tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 4: Tokenization using SpaCy (English and Multilingual)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# English Tokenization with SpaCy\n",
    "nlp_en = spacy.blank(\"en\")\n",
    "text_en = \"\"\"There are multiple ways we can perform tokenization on given text data.\"\"\"\n",
    "doc_en = nlp_en(text_en)\n",
    "tokens_en = [token.text for token in doc_en]\n",
    "print(\"Tokens with SpaCy (English):\", tokens_en)\n",
    "\n",
    "# Hindi Tokenization with SpaCy\n",
    "nlp_hi = spacy.blank(\"hi\")\n",
    "text_hi = \"\"\"ऐसे कई तरीके हैं जिनसे हम दिए गए टेक्स्ट डेटा पर टोकनाइजेशन कर सकते हैं।\"\"\"\n",
    "doc_hi = nlp_hi(text_hi)\n",
    "tokens_hi = [token.text for token in doc_hi]\n",
    "print(\"Tokens with SpaCy (Hindi):\", tokens_hi)\n",
    "\n",
    "# Gujarati Tokenization with SpaCy\n",
    "nlp_gu = spacy.blank(\"gu\")\n",
    "text_gu = \"\"\"આપેલ ટેક્સ્ટ ડેટા પર આપણે ટોકનાઇઝેશન કરી શકીએ તે ઘણી રીતો છે.\"\"\"\n",
    "doc_gu = nlp_gu(text_gu)\n",
    "tokens_gu = [token.text for token in doc_gu]\n",
    "print(\"Tokens with SpaCy (Gujarati):\", tokens_gu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy is a powerful tool for tokenization, supporting multiple languages. Here we use it to tokenize English, Hindi, and Gujarati texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 5: Enhancements</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2599ef746c5541729a84cc003cfd1896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultra-fast tokenization (Hugging Face): ['[CLS]', 'there', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'token', '##ization', 'on', 'given', 'text', 'data', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aashi\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aashi\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data.\"\"\"\n",
    "output = tokenizer.encode(text)\n",
    "\n",
    "print(\"Ultra-fast tokenization (Hugging Face):\", output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultra-fast tokenization often refers to using libraries that offer optimized tokenization at scale. A library like Hugging Face's tokenizers` is highly optimized for speed and should be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
